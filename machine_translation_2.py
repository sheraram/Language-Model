# -*- coding: utf-8 -*-
"""2019101034_MT-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-3K29WEaLwey0X7UWs049ALBdzB2gU_s
"""

! pip install torchtext
! pip freeze
! python -m spacy download en --quiet
! python -m spacy download fr --quiet

import numpy as np
import pandas as pd
import pickle
import time
import math
import spacy
import os
import torch
import torchtext
from torchtext.vocab import vocab
import torch.optim as optim
import torch.nn as nn

from torchtext.data.utils import get_tokenizer
from unicodedata import normalize
from collections import Counter
from torchtext.legacy.data import Field, BucketIterator,TabularDataset
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

english = spacy.load('en')
french = spacy.load('fr')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Colab Notebooks/NLP_A3
!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd ted-talks-corpus
!ls

with open("train.en", 'r') as inFile:
    en_train = inFile.readlines()
inFile.close()

with open("train.fr", 'r') as inFile:
    fr_train = inFile.readlines()
inFile.close()


with open("dev.en", 'r') as inFile:
    en_dev = inFile.readlines()
inFile.close()

with open("dev.fr", 'r') as inFile:
    fr_dev = inFile.readlines()
inFile.close()


with open("test.en", 'r') as inFile:
    en_test = inFile.readlines()
inFile.close()

with open("test.fr", 'r') as inFile:
    fr_test = inFile.readlines()
inFile.close()

"""##Data Preparing"""

punct = '''!()-[]{};:'"\,<>./?@#$%^&*_~,'''
def RemovePunct(text):
  data=[]
  for i in text:
    for j in i:
      if j in punct:
          i=i.replace(j," ")
    data.append(i)
  return data

en_train = RemovePunct(en_train)
fr_train = RemovePunct(fr_train)
en_dev   = RemovePunct(en_dev)
fr_dev   = RemovePunct(fr_dev)
en_test  = RemovePunct(en_test)
fr_test  = RemovePunct(fr_test)

# en_train = RemovePunct(en_train[:100])
# fr_train = RemovePunct(fr_train[:100])
# en_dev   = RemovePunct(en_dev[:100])
# fr_dev   = RemovePunct(fr_dev[:100])
# en_test  = RemovePunct(en_test[:100])
# fr_test  = RemovePunct(fr_test[:100])

train = pd.DataFrame({'srg': en_train, 'trg': fr_train})
test  = pd.DataFrame({'srg': en_test,  'trg': fr_test})
dev   = pd.DataFrame({'srg': en_dev,   'trg': fr_dev})

train.to_csv("train.csv", index = False)
test.to_csv("test.csv", index = False)
dev.to_csv("dev.csv", index = False)

train

test

def en_tokenizer(text):
  # data = [token.text for token in english.tokenizer(text)]
  data =[]
  text = english.tokenizer(text)
  for token in text:
    data.append(token.text)
  return data

def fr_tokenizer(text):
  # data = [token.text for token in french.tokenizer(text)]
  data =[]
  text = french.tokenizer(text)
  for token in text:
    data.append(token.text)
  return data

english1 = Field(tokenize = en_tokenizer,
               lower = True,
               init_token = "<sos>",
               eos_token = "<eos>")

french1 = Field(tokenize = fr_tokenizer,
               lower = True,
               init_token = "<sos>",
               eos_token = "<eos>")

print(english1.__dict__.keys())

train_data, valid_data, test_data = TabularDataset.splits(
    path= "./",
    train = "train.csv", 
    validation="dev.csv", 
    test = "test.csv", 
    format = "csv",
    fields = [('src', english1), ('trg', french1)],
    skip_header = True
)

english1.build_vocab(train_data)
french1.build_vocab(train_data)

e_vocab = english1.vocab
f_vocab = french1.vocab

print("English Vocabulary : ",e_vocab.itos[:100])
print("English Vocab length ",len(english1.vocab.itos))
print(english1.vocab.stoi['are'])
print(e_vocab.stoi['shera'])
print(english1.vocab["student"])
'adjourned' in e_vocab.itos

print("French Vocabulary : ",f_vocab.itos[:100])
print("French Vocab length ",len(french1.vocab.itos))
print(french1.vocab.stoi['un'])
print(f_vocab.stoi['shera'])
print(french1.vocab["Ã©tudiante"])
'adjourned' in f_vocab.itos

print(english1.__dict__.keys())
print(french1.__dict__.keys())

batch_size = 32
train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), 
                                                                      batch_size = batch_size, 
                                                                      sort_within_batch = True,
                                                                      sort_key = lambda x: len(x.src),
                                                                      device = device)



"""##Encode - Decoder

Encoder
"""

class Encoder(nn.Module):
  def __init__(self, vocab_size, num_layers, embedding_size, hidden_size, dropout):
      super(Encoder,self).__init__()

      self.dropout = nn.Dropout(dropout)
      self.embedding = nn.Embedding(vocab_size, embedding_size)
      self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)

  def forward(self,input):
      embedding = self.dropout(self.embedding(input))
      out, hidden = self.lstm(embedding)
      hidden_state, cell_state = hidden[0], hidden[1]
      return hidden_state, cell_state

"""    Decoder"""

class Decoder(nn.Module):
  def __init__(self, input_size, num_layers, embedding_size, hidden_size, dropout, output_size):
    super(Decoder,self).__init__()

    self.input_size = input_size
    self.embedding_size = embedding_size
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.output_size = output_size
    self.dropout = nn.Dropout(dropout)

    self.embedding = nn.Embedding(self.input_size, self.embedding_size)
    self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, dropout = dropout)
    self.fc = nn.Linear(self.hidden_size, self.output_size)

  def forward(self, input,hidden_state, cell_state):
    # input = input.unsqueeze(0)
    embedding = self.dropout(self.embedding(input))
    out, hidden = self.lstm(embedding, (hidden_state, cell_state))
    hidden_state, cell_state = hidden[0], hidden[1]
    predictions = self.fc(out).squeeze(0)
    # predictions = prediction.squeeze(0)
    return predictions, hidden_state, cell_state

# vocab_size=len(english1.vocab.itos) # for encoder
# vocab_size=len(english1.vocab.itos) # for Decoder
# embedding_size=300
# hidden_size=512
# dropout=0.5


# Encoder(vocab_size,num_layers,embedding_size,hidden_size,dropout).to(device)
encoder_lstm = Encoder(len(english1.vocab.itos), 1, 300, 512, 0.5).to(device)
print(encoder_lstm)

# Decoder(vocab_size_f, num_layers,embedding_size,hidden_size,dropout,vocab_size_f).to(device)
decoder_lstm=Decoder(len(french1.vocab.itos), 1, 300, 512, 0.5, len(french1.vocab.itos)).to(device)
print(decoder_lstm)

input=torch.load("lm_f")
output=torch.load("lm_e")

encoder_lstm.state_dict()["lstm.weight_ih_l0"][:]=input['lstm.weight_ih_l0']
encoder_lstm.state_dict()["lstm.weight_hh_l0"][:]=input['lstm.weight_hh_l0']
encoder_lstm.state_dict()["lstm.bias_ih_l0"][:]=input['lstm.bias_ih_l0']
encoder_lstm.state_dict()["lstm.bias_hh_l0"][:]=input['lstm.bias_hh_l0']

decoder_lstm.state_dict()["lstm.weight_ih_l0"][:]=output['lstm.weight_ih_l0']
decoder_lstm.state_dict()["lstm.weight_hh_l0"][:]=output['lstm.weight_hh_l0']
decoder_lstm.state_dict()["lstm.bias_ih_l0"][:]=output['lstm.bias_ih_l0']
decoder_lstm.state_dict()["lstm.bias_hh_l0"][:]=output['lstm.bias_hh_l0']



"""    Seq to Seq"""

import random
class Seq2Seq(nn.Module):
  def __init__(self, Encoder_LSTM, Decoder_LSTM):
    super(Seq2Seq, self).__init__()
    self.Encoder_LSTM = Encoder_LSTM
    self.Decoder_LSTM = Decoder_LSTM

  def forward(self, source, target, tfr=0.5):
    outputs = torch.zeros(target.shape[0], source.shape[1], len(french1.vocab.itos)).to(device)
    
    hidden_state, cell_state = self.Encoder_LSTM(source)

    x = target[0] # Trigger token <SOS>

    for i in range(1, target.shape[0]):
      output, hidden_state, cell_state = self.Decoder_LSTM(x.unsqueeze(0), hidden_state, cell_state)
      outputs[i] = output

      # 0th dimension is batch size, 1st dimension is word embedding
      best_guess = output.argmax(1) 

      # Either pass the next word correctly from the dataset or use the earlier predicted word
      x = target[i] if random.random() < tfr else best_guess 
    return outputs

learning_rate = 1e-3

model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)
optimizer = optim.Adam(model.parameters(), lr = learning_rate)
pad_ids = french1.vocab.stoi["<pad>"]
criterion = nn.CrossEntropyLoss(ignore_index=pad_ids)

# model=Seq2Seq(encoder_lstm, decoder_lstm).to(device)
# optimizer = optim.Adam(Seq2Seq(encoder_lstm, decoder_lstm).to(device).parameters(),lr = learning_rate)

# print(model)

# model=Seq2Seq(encoder_lstm, decoder_lstm).to(device)

for name, param in model.named_parameters():
    nn.init.uniform_(param.data, -0.08, 0.08)

def write(epoch, train_loss, valid_loss):
  # mins, secs = epoch_time(start_time, end_time)
  print(f'Epoch: {epoch+1:02}')
  print(f'\tTrain Perplexity: {math.exp(train_loss):.3f}')
  print(f'\tValid Perplexity: {math.exp(valid_loss):.3f}')

epochs = 50
best_loss=float('inf')
best_epoch = -1

for epoch in range(epochs):
  print(f'Epoch: {epoch+1}/{epochs} ')

  epochs_loss_training = 0.0
  epochs_loss_validating = 0.0

  model.train()
  for idx, batch in enumerate(train_iterator):
    # input, target = batch.src.to(device), batch.trg.to(device)
    input = batch.src.to(device)
    target = batch.trg.to(device)
    output = model.forward(input, target)
    output, target = output[1:].reshape(-1, output.shape[2]), target[1:].reshape(-1)

    loss = criterion(output,target)

    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)

    optimizer.step()
    epochs_loss_training+= loss.item()

  print(f'\tAverage Training loss : {epochs_loss_training/len(train_iterator)}')
  # print(epochs_loss_training/len(train_iterator))

  model.eval()
  for idx,bat in enumerate(valid_iterator):
    input, target = bat.src.to(device), bat.trg.to(device)
   
    output = model(input,target)
    output, target = output[1:].reshape(-1,output.shape[2]), target[1:].reshape(-1)

    loss = criterion(output, target)
    epochs_loss_validating+=loss.item()

  if best_loss > epochs_loss_validating:
    best_loss = epochs_loss_validating
    state={
        "model":model.state_dict(),
        "optimizer":optimizer.state_dict(),
        "rng_state":torch.get_rng_state(),
    }
    torch.save(state,'MT-1')
    best_epoch = epoch
  print(f'\tAverage Validation loss : {epochs_loss_validating/len(valid_iterator)}')
  # print("Average Validation loss...", epochs_loss_validating/len(valid_iterator))
  print(f'\tepoch_loss : {loss.item()}')

print(f'Best Epoch : {best_epoch}')

model.load_state_dict(torch.load("MT-2")['model'])

"""###Save to text file"""

import os

try:
  os.remove("2019101034_MT2_test.txt")
except:
  pass

def translate(en_data, fr_data):
  bleu_score=[]
  avg_score=0
  french_actual=[]
  french_predict=[]

  l = len(en_data)
  for i in range(l):
    predict_sent = Transalate(model, sent, english1, french1, device, 50)
    actual_sent = fr_tokenizer(fr_data[i])

    french_actual.append(actual_sent)
    french_predict.append(predict_sent)

    score = sentence_bleu(predict_sent, actual_sent, weights=(1,0,0,0))
    bleu_score.append([" ".join(predict_sent), score])
    
    avg_score+= score
  
  return bleu_score, avg_score, french_actual, french_predict

bleu_score, avg_score, french_actual, french_predict = translate(en_train, fr_train)

try:
  os.remove("2019101034_MT2_train.txt")
except:
  pass

with open("2019101034_MT2_train.txt","a") as file:
  file.write(str(corpus_bleu(french_actual, french_predict)))
  file.write("\n")
  for i in bleu_score:
    string=str(i[0])+"  "+str(i[1])
    file.write(string)
    file.write("\n")

bleu_score, avg_score, french_actual, french_predict = translate(en_test, fr_test)

try:
  os.remove("2019101034_MT2_test.txt")
except:
  pass

with open("2019101034_MT2_test.txt","a") as file:
  file.write(str(corpus_bleu(french_actual, french_predict)))
  file.write("\n")
  for i in bleu_score:
        string=str(i[0])+"  "+str(i[1])
        file.write(string)
        file.write("\n")