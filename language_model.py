# -*- coding: utf-8 -*-
"""2019101034_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orr0MiORiutGfpKW7i5jVn8CiSwXs9xX
"""

import numpy as np
import pandas as pd
import torchtext
import torch
from torchtext.vocab import vocab
import time
import math
from torchtext.data.utils import get_tokenizer

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from torchtext.legacy.data import Field, BucketIterator,TabularDataset

! pip install -U spacy
!python -m spacy download fr_core_news_sm
import spacy
fren_tokenizer=spacy.load('fr_core_news_sm')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Colab Notebooks/NLP_A3
!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd europarl-corpus
!ls

with open("train.europarl", 'r') as inFile:
    appreciate_data = inFile.readlines()
inFile.close()
len(appreciate_data)

with open("train.europarl", 'r') as inFile:
    en_train = inFile.readlines()
inFile.close()

with open("dev.europarl", 'r') as inFile:
    en_dev = inFile.readlines()
inFile.close()

with open("test.europarl", 'r') as inFile:
    en_test = inFile.readlines()
inFile.close()

# English
tokenizer1 = get_tokenizer('basic_english')

def en_tokenizer(text):
  data =[]
  text = tokenizer1(text)
  # for token in text:
    # data.append(token.text)
  return text
  
e_tokenized_dataset=[]
for line in appreciate_data:
  e_tokenized_dataset.append(tokenizer1(line))
print(len(e_tokenized_dataset))


# French
tokenizer2 = get_tokenizer('spacy', language='fr')

def fr_tokenizer(text):
  data =[]
  text = tokenizer2(text)
  # for token in text:
  #   data.append(token.text)
  return data

f_tokenized_dataset=[]
for line in appreciate_data:
  f_tokenized_dataset.append(tokenizer2(line))
print(len(f_tokenized_dataset))

english1 = Field(tokenize = en_tokenizer,
               lower = True,
               init_token = "<sos>",
               eos_token = "<eos>")

french1 = Field(tokenize = fr_tokenizer,
               lower = True,
               init_token = "<sos>",
               eos_token = "<eos>")

e_tokenized_dataset
f_tokenized_dataset

train_data, valid_data, test_data = TabularDataset.splits(
    path= "./",
    train = "train.csv", 
    validation="dev.csv", 
    test = "test.csv", 
    format = "csv",
    fields = [('src', english1)],
    skip_header = True,
)

english1.build_vocab(train_data)
e_vocab = english1.vocab

french1.build_vocab(train_data)
f_vocab = french1.vocab

print("Vocabulary : ",e_vocab.itos[:100])
print(english1.vocab.stoi['adjourned'])
print(e_vocab.stoi['shera'])
'adjourned' in e_vocab.itos

# Not working properly because it not takes words different than dataset(which are not in dataset)

# e_vocab = torchtext.vocab.build_vocab_from_iterator(
#     e_tokenized_dataset, 
#     specials=["<unk>","<eos>"]
#     )

# print("Vocabulary : ",e_vocab.get_itos()[:100])
# 'adjourned' in e_vocab.get_itos()

def get_data(dataset, vocab, batch_size):
  data = []
  # i=1
  for sent in dataset:
    sent1= sent.copy()
    sent1.insert(0, '<unk>')
    sent1.append('<eos>')
    tokens = []
    for word in sent1:
      x = vocab.stoi[word]
      tokens.append(x)
    data.extend(tokens)
  #   i+=1
  #   if i==50:
  #     break
  # print("breaked")
  data = torch.LongTensor(data)
  n_batches = data.shape[0] // batch_size
  tot_val = n_batches * batch_size
  data = data.narrow(0, 0, tot_val)
  data = data.view(batch_size, -1)

  return data

batch_size = 16
e_train_data=get_data(e_tokenized_dataset, e_vocab, batch_size)
# e_train_data=get_data(e_tokenized_dataset, e_vocab, batch_size)
# f_train_data=get_data(f_tokenized_dataset,f_vocab, batch_size)

print(e_train_data.shape)
print(e_train_data[:105])
# train_data.shape[-1]

import torch.nn as nn
import torch.optim as optim

class Model(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):
    super().__init__()
    self.n_layers = n_layers
    self.hidden_dim = hidden_dim
    self.vocab_size = vocab_size
    self.embedding_dim = embedding_dim
    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)
    self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, num_layers=1, dropout=dropout_rate,batch_first=True)
    self.fc = nn.Linear(self.hidden_dim, self.vocab_size)
    self.dropout = nn.Dropout(dropout_rate)

  def init_hidden(self, batch_size, device):
    hidden = torch.zeros(1, batch_size, self.hidden_dim).to(device)
    cell = torch.zeros(1, batch_size, self.hidden_dim).to(device)
    return hidden, cell

  def detach_hidden(self, hidden):
    hidden, cell = hidden
    return hidden.detach(), cell.detach()

  def forward(self, input, hidden):
    embedding = self.dropout(self.embedding(input))
    # print("forward - hidden : ",hidden[0].shape,hidden[1].shape)
    
    output, hidden = self.lstm(embedding, hidden)
    # output = self.dropout(output)
    prediction = self.fc(self.dropout(output))
    prediction = self.fc(output)
    return prediction, hidden

def get_batch(data, max_seq_len, tokens, offset):
  seq_len = min(max_seq_len, tokens - offset -1)
  seq_end_pos = offset + seq_len

  input = data[:, offset:seq_end_pos]
  target = data[:, offset+1 : seq_end_pos + 1]

  return input, target, seq_len

def trainmodel(model, data, optimizer, criterion, batch_size, max_seq_len, device):
  epoch_loss = 0
  model.train()
  n_tokens = data.shape[-1]
  hidden = model.init_hidden(batch_size, device)

  for offset in range(0, n_tokens-1 ,max_seq_len):
    optimizer.zero_grad()
    input, target, seq_len = get_batch(data, max_seq_len, n_tokens,offset)
    input = input.to(device)
    batch_size , seq_len = input.shape
    
    hidden = model.detach_hidden(hidden)


    prediction, hidden = model(input, hidden)
    prediction = prediction.reshape(batch_size*seq_len, -1)
    target = target.to(device).reshape(-1)

    loss = criterion(prediction, target)

    loss.backward()
    optimizer.step()

    epoch_loss+= loss.item()*seq_len
  return epoch_loss/n_tokens

# torch.Size([15]) torch.Size([15])
# HAHAHA
# torch.Size([15]) torch.Size([15])
# torch.Size([1, 16, 1024]) torch.Size([1, 16, 1024])

def evalutemodel(model, data, criterion, batch_size, max_seq_len, device):
  epoch_loss = 0
  model.train()
  hidden, cell = model.init_hidden(batch_size, device)
  n_tokens = data.shape[-1]

  with torch.no_grad():
    for offset in range(0, n_tokens, max_seq_len):
      input,target,seq_len=get_batch(data, max_seq_len, n_tokens, offset)

      input = input.to(device)
      target = target.to(device)
      batch_size , seq_len = input.shape

      prediction, (hidden, cell) = model(input, (hidden,cell))
      prediction = prediction.reshape(batch_size*seq_len, -1)

      target = target.reshape(-1)
      loss = criterion(prediction,target)
      epoch_loss+= loss.item()*seq_len

  return epoch_loss/n_tokens

def epoch_time(start_time, end_time):
  elapsed_time = end_time - start_time
  return int(elapsed_time / 60), int(elapsed_time - (int(elapsed_time / 60) * 60))

def write(epoch, train_loss, valid_loss, start_time, end_time):
  mins, secs = epoch_time(start_time, end_time)
  print(f'Epoch: {epoch+1:02} | Epoch Time: {mins}m {secs}s')
  print(f'\tTrain Perplexity: {math.exp(train_loss):.3f}')
  print(f'\tValid Perplexity: {math.exp(valid_loss):.3f}')

# self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate
learning_rate = 0.001
vocab_size = len(e_vocab)
print(vocab_size)
embedding_dim = 1024
hidden_dim = 1024
n_layers = 2
dropout_rate = 0.65

# (self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate):
model = Model(vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate)

parameters= sum(p.numel() for p in model.parameters() if p.requires_grad)

print("The Model has ",parameters," trainable parameters")

optimizer = optim.Adam(model.parameters(), lr = learning_rate)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
criterion = nn.CrossEntropyLoss().to(device)

n_epochs = 5
max_seq_len = 50
clip = 0.25
best_loss = float('inf')

for epoch in range(n_epochs):
  start_time = time.monotonic()
  train_loss = trainmodel(
      model, 
      e_train_data, 
      optimizer, 
      criterion, 
      batch_size, 
      max_seq_len, 
      device
  )
  valid_loss = evalutemodel(model, e_train_data, criterion, batch_size, max_seq_len, device)
  end_time = time.monotonic()
  write(epoch, train_loss, valid_loss, start_time, end_time)
  if valid_loss < best_loss:
    best_loss = valid_loss
    torch.save(model.state_dict(), 'lm_en.pt')

model.load_state_dict(torch.load('lm_en.pt'))

def Preplexity(model, sent, batch_size, size, prob, temperature):
  sum= 0 
  indices = []
  l = len(sent)
  for token in sent:
    x = e_vocab.stoi[token]
    indices.append(x)
  # for i in range(l-1):
  #   print(indices[:i+1])

  hidden = model.init_hidden(batch_size, device)

  with torch.no_grad():
    for i in range(l-1):
      input = torch.LongTensor([indices[:i+1]]).to(device)
      prediction, hidden = model(input, hidden)

      probs = torch.softmax(prediction[:, -1] / temperature, dim =-1) 
      prob*= probs[0, indices[i+1]].item()
      sum+=math.log(probs[0, indices[i+1]].item())
  
  return math.exp(-1*(sum/l)), prob

sent = " I am a man."
sent1 = tokenizer1(sent)
batch_size = 1
size= 0
prob = 1
temperature = 0.5

df = Preplexity(model, sent1, batch_size, size, prob, temperature)

print(df)

Dict = dict([
             ('Probbality ',df[1]),
             ('Preplexity ',df[0]),
])

Dict

with open("Q1_Model.txt", 'w') as f: 
    for key, value in Dict.items(): 
        f.write('%s: %s\n' % (key, value))

# Preplexity for train data


preplexity_store = []
avg_preplexity_score = 0
i=1
for sent in en_train:
  i+=1
  print(sent)
  sent1 = tokenizer1(sent)
  df = Preplexity(model, sent1, batch_size, size, prob, temperature)
  preplexity_store.append([sent, df[0]])
  avg_preplexity_score += df[0]
  if i is 10:
    break

with open("2019101034_LM_train_pre.txt", "a") as file:
    file.write(str(avg_preplexity_score/len(en_train)))
    file.write("\n")
    for i in preplexity_store:
        string = str(i[0])+" -> "+str(i[1])
        file.write(string)
        file.write("\n")

# Preplexity for test data

preplexity_store = []
avg_preplexity_score = 0
i=1
for sent in en_test:
  i+=1
  print(sent)
  sent1 = tokenizer1(sent)
  df = Preplexity(model, sent1, batch_size, size, prob, temperature)
  preplexity_store.append([sent, df[0]])
  avg_preplexity_score += df[0]
  if i is 10:
    break

with open("2019101034_LM_test_pre.txt", "a") as file:
    file.write(str(avg_preplexity_score/len(en_test)))
    file.write("\n")
    for i in preplexity_store:
        string = str(i[0])+" -> "+str(i[1])
        file.write(string)
        file.write("\n")